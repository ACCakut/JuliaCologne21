{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation (AD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, the promise of AD is\n",
    "\n",
    "```julia\n",
    "f(x) = 4x + x^2\n",
    "\n",
    "df(x) = derivative(f, x)\n",
    "```\n",
    "\n",
    "such that\n",
    "\n",
    "```julia\n",
    "df(3) = 4 + 2*3 = 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What AD is not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symbolic rewriting:**\n",
    "$$ f(x) = 4x + x^2 \\quad \\rightarrow \\quad df(x) = 4 + 2x $$\n",
    "\n",
    "**Numerical differentiation:**\n",
    "$$ \\frac{df}{dx} \\approx \\frac{f(x+h) - f(x)}{\\Delta h} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key to AD is the application of the chain rule\n",
    "$$\\dfrac{d}{dx} f(g(x)) = \\dfrac{df}{dg} \\dfrac{dg}{dx}$$\n",
    "\n",
    "Consider the function $f(a,b) = \\ln(ab + \\sin(a))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b) = log(a*b + sin(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_derivative(a,b) = 1/(a*b + sin(a)) * (b + cos(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3.1\n",
    "b = 2.4\n",
    "f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the function into the elementary steps, it corresponds to the following \"*computational graph*\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/comp_graph.svg\" width=300px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_graph(a,b)\n",
    "    c1 = a*b\n",
    "    c2 = sin(a)\n",
    "    c3 = c1 + c2\n",
    "    c4 = log(c3)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b) == f_graph(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate $\\frac{\\partial f}{\\partial a}$ we have to apply the chain rule multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dfrac{\\partial f}{\\partial a} = \\dfrac{\\partial f}{\\partial c_4} \\dfrac{\\partial c_4}{\\partial a} = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\dfrac{\\partial c_3}{\\partial a}  \\right) = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\left( \\dfrac{\\partial c_3}{\\partial c_2} \\dfrac{\\partial c_2}{\\partial a} + \\dfrac{\\partial c_3}{\\partial c_1} \\dfrac{\\partial c_1}{\\partial a}\\right)  \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_graph_derivative(a,b)\n",
    "    c1 = a*b\n",
    "    c1_ϵ = b\n",
    "    \n",
    "    c2 = sin(a)\n",
    "    c2_ϵ = cos(a)\n",
    "    \n",
    "    c3 = c1 + c2\n",
    "    c3_ϵ = c1_ϵ + c2_ϵ\n",
    "    \n",
    "    c4 = log(c3)\n",
    "    c4_ϵ = 1/c3 * c3_ϵ\n",
    "    \n",
    "    c4, c4_ϵ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_graph_derivative(a,b)[2] == f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can we automate this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D for \"dual number\", invented by Clifford in 1873.\n",
    "struct D <: Number\n",
    "    x::Float64 # value\n",
    "    ϵ::Float64 # derivative\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base: +, *, /, -, sin, log, convert, promote_rule\n",
    "\n",
    "a::D + b::D = D(a.x + b.x, a.ϵ + b.ϵ) # sum rule\n",
    "a::D - b::D = D(a.x - b.x, a.ϵ - b.ϵ)\n",
    "a::D * b::D = D(a.x * b.x, a.x * b.ϵ + a.ϵ * b.x) # product rule\n",
    "a::D / b::D = D(a.x / b.x, (b.x * a.ϵ - a.x * b.ϵ)/b.x^2) # quotient rule\n",
    "\n",
    "sin(a::D) = D(sin(a.x), cos(a.x) * a.ϵ)\n",
    "log(a::D) = D(log(a.x), 1/a.x * a.ϵ)\n",
    "\n",
    "Base.convert(::Type{D}, x::Real) = D(x, zero(x))\n",
    "Base.promote_rule(::Type{D}, ::Type{<:Number}) = D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(D(a,1), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! That was easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(D(a,1), b).ϵ ≈ f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does this work?!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick of forward mode AD is to let Julia implicitly perform the mapping `f -> f_graph_derivative` for you and then let the compiler optimize the resulting code structure (that's what compilers do!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_typed f(D(a,1), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is somewhat hard to parse, plugging these operations manually into each other we find that this code equals\n",
    "\n",
    "```julia\n",
    "D.x = log(a.x*b + sin(a.x))\n",
    "D.ϵ = 1/(a.x*b + sin(a.x)) * (a.x*0 + (a.ϵ*b) + cos(a.x)*a.ϵ)\n",
    "```\n",
    "\n",
    "which, if we drop `a.x*0`, set `a.ϵ = 1`, and rename `a.x` $\\rightarrow$ `a`, reads\n",
    "\n",
    "```julia\n",
    "D.x = log(a*b + sin(a))\n",
    "D.ϵ = 1/(a*b + sin(a)) * (b + cos(a)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This precisely matches our definitions from above:\n",
    "\n",
    "```julia\n",
    "f(a,b) = log(a*b + sin(a))\n",
    "\n",
    "f_derivative(a,b) = 1/(a*b + sin(a)) * (b + cos(a))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, the compiler sees the entire \"rewritten\" code and can therefore apply optimizations. In this simple example, we find that the code produced by our simple Forward mode AD is essentially identical to the explicit implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none f_graph_derivative(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none f(D(a,1), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our AD is alreadly pretty powerful and general. Let's define the promised function `derivative`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(f::Function, x::Number) = f(D(x, one(x))).ϵ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x) = x + x^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(g, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymous function oft come in handy here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->3*x^2+4x+5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->sin(x)*log(x), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define the partial derivative $\\frac{df(a,b)}{da}$ from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(x) = derivative(a->f(a,b),x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `b` is \"wrapped into a closure\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(1.23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking the derivative of *code*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Repeat $t \\leftarrow (t + x/2)/2$ until $t$ converges to $\\sqrt{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline function Babylonian(x; N = 10)\n",
    "    t = (1+x)/2\n",
    "    for i = 2:N\n",
    "        t = (t + x/t)/2\n",
    "    end\n",
    "    t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our forward mode AD, that is our dual numbers, we can compute the derivative of `Babylonian` **with no rewrite at all**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(D(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / (2*sqrt(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It just works and is efficient!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none Babylonian(D(5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursion? Works as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function power(x, n)\n",
    "    if n <= 0\n",
    "        return 1\n",
    "    else\n",
    "        return x*power(x, n-1)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.0^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x -> power(x,3), 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3*4.0^2 # 3*x^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deriving our Vandermonde matrix from yesterday?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vander_generic(x::AbstractVector{T}) where T\n",
    "    m = length(x)\n",
    "    V = Matrix{T}(undef, m, m)\n",
    "    for j = 1:m\n",
    "        V[j,1] = one(x[j])\n",
    "    end\n",
    "    for i= 2:m\n",
    "        for j = 1:m\n",
    "            V[j,i] = x[j] * V[j,i-1]\n",
    "            end\n",
    "        end\n",
    "    return V\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}V=\\begin{bmatrix}1&a&a^{2} &a^3\\\\1&b&b^{2} &b^3\\\\1&c&c^{2} &c^3\\\\1&d&d^{2} &d^3\\end{bmatrix}\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{dV}{da}=\\begin{bmatrix}0&1&2a &3a^2\\\\0&0&0 &0\\\\0&0&0 &0\\\\0&0&0 &0\\end{bmatrix}\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d = 2, 3, 4, 5\n",
    "V = vander_generic([D(a,1), D(b,0), D(c,0), D(d,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[V[i,j].ϵ for i in axes(V,1), j in axes(V,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolically (because we can)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is mathematically equivalent, **though not exactly what the computation is doing**. Our AD isn't performing symbolic manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SymPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vars x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(x; N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(Babylonian(x; N=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify(Babylonian(x; N=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify(diff(simplify(Babylonian(x; N=5)), x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't reinvent the wheel: ForwardDiff.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have understood how forward AD works, we can use the more feature complete package [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@edit ForwardDiff.derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: [DiffRules.jl](https://github.com/JuliaDiff/DiffRules.jl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If time permits: Reverse mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward mode:\n",
    "$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial c_4} \\dfrac{\\partial c_4}{\\partial x} = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\dfrac{\\partial c_3}{\\partial x}  \\right) = \\dfrac{\\partial f}{\\partial c_4} \\left( \\dfrac{\\partial c_4}{\\partial c_3} \\left( \\dfrac{\\partial c_3}{\\partial c_2} \\dfrac{\\partial c_2}{\\partial x} + \\dfrac{\\partial c_3}{\\partial c_1} \\dfrac{\\partial c_1}{\\partial x}\\right)  \\right)$\n",
    "\n",
    "Reverse mode:\n",
    "$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial c_4} \\dfrac{\\partial c_4}{\\partial x} = \\left( \\dfrac{\\partial f}{\\partial c_3}\\dfrac{\\partial c_3}{\\partial c_4}   \\right) \\dfrac{\\partial c_4}{\\partial x} = \\left( \\left( \\dfrac{\\partial f}{\\partial c_2} \\dfrac{\\partial c_2}{\\partial c_3} + \\dfrac{\\partial f}{\\partial c_1} \\dfrac{\\partial c_1}{\\partial c_3} \\right) \\dfrac{\\partial c_3}{\\partial c_4} \\right) \\dfrac{\\partial c_4}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward mode AD requires $n$ passes in order to compute an $n$-dimensional\n",
    "gradient.\n",
    "\n",
    "Reverse mode AD requires only a single run in order to compute a complete gradient but requires two passes through the graph: a forward pass during which necessary intermediate values are computed and a backward pass which computes the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rule of thumb:*\n",
    "\n",
    "Forward mode is good for $\\mathbb{R} \\rightarrow \\mathbb{R}^n$ while reverse mode is good for $\\mathbb{R}^n \\rightarrow \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An efficient source-to-source reverse mode AD is implemented in [Zygote.jl](https://github.com/FluxML/Zygote.jl), the AD underlying [Flux.jl](https://fluxml.ai/) (since version 0.10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 5*x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(f, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none gradient(f,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none derivative(f,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some nice reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers:\n",
    "* https://www.jmlr.org/papers/volume18/17-468/17-468.pdf\n",
    "\n",
    "Lectures:\n",
    "\n",
    "\n",
    "* https://mitmath.github.io/18337/lecture8/automatic_differentiation.html\n",
    "\n",
    "Blog posts:\n",
    "\n",
    "* ML in Julia: https://julialang.org/blog/2018/12/ml-language-compiler\n",
    "\n",
    "* Nice example: https://fluxml.ai/2019/03/05/dp-vs-rl.html\n",
    "\n",
    "* Nice interactive examples: https://fluxml.ai/experiments/\n",
    "\n",
    "* Why Julia for ML? https://julialang.org/blog/2017/12/ml&pl\n",
    "\n",
    "* Neural networks with differential equation layers: https://julialang.org/blog/2019/01/fluxdiffeq\n",
    "\n",
    "* Implement Your Own Automatic Differentiation with Julia in ONE day : http://blog.rogerluo.me/2018/10/23/write-an-ad-in-one-day/\n",
    "\n",
    "* Implement Your Own Source To Source AD in ONE day!: http://blog.rogerluo.me/2019/07/27/yassad/\n",
    "\n",
    "Repositories:\n",
    "\n",
    "* AD flavors, like forward and reverse mode AD: https://github.com/MikeInnes/diff-zoo (Mike is one of the smartest Julia ML heads)\n",
    "\n",
    "Talks:\n",
    "\n",
    "* AD is a compiler problem: https://juliacomputing.com/assets/pdf/CGO_C4ML_talk.pdf"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
