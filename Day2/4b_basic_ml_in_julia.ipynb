{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network in ~10 lines of Julia code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a fully-connected two layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense(W, b, σ = identity) = x -> σ.(W * x .+ b)\n",
    "\n",
    "chain(layers...) = foldl(∘, reverse(layers))\n",
    "\n",
    "network = chain(\n",
    "    dense(randn(5,3), randn(5), tanh), # 3 input neurons  -> 5 hidden neurons\n",
    "    dense(randn(2,5), randn(2)))       # 5 hidden neurons -> 2 output neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(3); # some input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unravel the compact definition of our neural network a bit:\n",
    "\n",
    "*Dense layer:*\n",
    "```julia\n",
    "dense(W, b, σ = identity) = x -> σ.(W * x .+ b)\n",
    "```\n",
    "`σ` is the activation function, `W` is the weight matrix, `x` is the input to the layer, and `b` are the biases.\n",
    "\n",
    "*Chaining layers:*\n",
    "```julia\n",
    "chain(layers...) = foldl(∘, reverse(layers))\n",
    "```\n",
    "The `layers...` means that the `chain` function takes an arbitrary number of layer (functions) as input. Those functions are then reversed (`reverse`) and composed (`∘`) via `foldl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = x^2\n",
    "g(x) = 2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f ∘ g)(3) # == f(g(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(g(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldl(∘, [f,g])(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network `network` is now just a piece of code, a function to be specific. We learned how to use automatic differentiation (AD) to derive functions/code. So let's do it! **Let's (fake) train our neural network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote # reverse mode AD\n",
    "\n",
    "# let's take `sum` as our cost function for now\n",
    "\n",
    "dnetwork = gradient(model->sum(model(x)), m)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training just amounts to updating the weights `W` and biases `b` according to the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.f.W # weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnetwork.f.W # weight gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "η = 0.01 # learning rate\n",
    "\n",
    "network.f.W .-= η * dnetwork.f.W # Gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flux - A pure-Julia machine learning library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web page: https://fluxml.ai/, Examples: [Model zoo](https://github.com/FluxML/model-zoo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://fluxml.ai/logo.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/flux.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chain(\n",
    "    Dense(3, 5, tanh),\n",
    "    Dense(5, 2),\n",
    "    softmax # normalize output neurons\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = rand(3, 100), fill(0.5, 2, 100); # fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(x, y) = sum(Flux.mse(m(x), y)) # mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Descent(0.01) # optimizer, i.e. gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss, params(m), [(data,labels)], opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(rand(3)) # trained model"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
